{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599414925763",
   "display_name": "Python 3.8.3 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<function tensorflow.python.framework.ops.enable_eager_execution(config=None, device_policy=None, execution_mode=None)>"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#from tqdm import tdqm\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv1DTranspose, Conv1D, Reshape, Input, ReLU, UpSampling1D, LeakyReLU\n",
    "import json\n",
    "import time\n",
    "tf.compat.v1.enable_eager_execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2 (None, 16, 16)\n3 (None, 64, 64)\n4 (None, 256, 4)\n5 (None, 1024, 2)\n6 (None, 4096, 1)\n7 (None, 16384, 1)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.engine.sequential.Sequential at 0x1c3a10f1040>"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "def makeGen():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Input(shape = (100,)))\n",
    "    model.add(Dense(256, use_bias= True))\n",
    "    model.add(Reshape((16,16)))\n",
    "    model.add(ReLU())\n",
    "    print(\"2\",model.output_shape)\n",
    "\n",
    "    model.add(Conv1DTranspose(64, kernel_size = 25,strides = 4, use_bias = True,padding ='same'))\n",
    "    model.add(ReLU())\n",
    "    print(\"3\",model.output_shape)\n",
    "    \n",
    "    model.add(Conv1DTranspose(4, kernel_size = 25,strides = 4, use_bias = True,padding ='same'))\n",
    "    model.add(ReLU())\n",
    "    print(\"4\",model.output_shape)\n",
    "\n",
    "    model.add(Conv1DTranspose(2, kernel_size = 25,strides = 4, use_bias = True,padding ='same'))\n",
    "    model.add(ReLU())\n",
    "    print(\"5\",model.output_shape)\n",
    "\n",
    "    model.add(Conv1DTranspose(1, kernel_size = 25,strides = 4, use_bias = True,padding ='same'))\n",
    "    model.add(ReLU())\n",
    "    print(\"6\",model.output_shape)\n",
    "\n",
    "    model.add(Conv1DTranspose(1, kernel_size = 25,strides = 4, use_bias = True,padding ='same'))\n",
    "    model.add(ReLU())\n",
    "    print(\"7\",model.output_shape)\n",
    "\n",
    "    model.add(Activation('tanh'))\n",
    "    return model\n",
    "makeGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 (None, 16384, 1)\n0 (None, 4096, 1)\n0 (None, 1024, 2)\n0 (None, 256, 4)\n0 (None, 64, 8)\n0 (None, 16, 16)\n0 (None, 256)\n0 (None, 1)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.engine.sequential.Sequential at 0x1c3a1151460>"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "def makeDisc():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Input(shape = (16384,1)))\n",
    "    print(\"0\",model.output_shape)\n",
    "\n",
    "    model.add(Conv1D(1,25,strides = 4, use_bias = True, padding = 'same'))\n",
    "    model.add(LeakyReLU(alpha = 0.2))\n",
    "    print(\"0\",model.output_shape)\n",
    "    model.add(Conv1D(2,25,strides = 4, use_bias = True, padding = 'same'))\n",
    "    model.add(LeakyReLU(alpha = 0.2))\n",
    "    print(\"0\",model.output_shape)\n",
    "    model.add(Conv1D(4,25,strides = 4, use_bias = True, padding = 'same'))\n",
    "    model.add(LeakyReLU(alpha = 0.2))\n",
    "    print(\"0\",model.output_shape)\n",
    "    model.add(Conv1D(8,25,strides = 4, use_bias = True, padding = 'same'))\n",
    "    model.add(LeakyReLU(alpha = 0.2))\n",
    "    print(\"0\",model.output_shape)\n",
    "    model.add(Conv1D(16,25,strides = 4, use_bias = True, padding = 'same'))\n",
    "    model.add(LeakyReLU(alpha = 0.2))\n",
    "    print(\"0\",model.output_shape)\n",
    "    model.add(Reshape((256,)))\n",
    "    print(\"0\",model.output_shape)\n",
    "    model.add(Dense(1))\n",
    "    print(\"0\",model.output_shape)\n",
    "    return model\n",
    "makeDisc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_path):\n",
    "    with open(dataset_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "    X = np.array(data[\"snares\"])\n",
    "    return X.astype(dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return -tf.reduce_mean(fake_output)\n",
    "\n",
    "def discriminator_loss(real_sample, fake_sample):\n",
    "    #print(real_sample, fake_sample)\n",
    "    real_loss = tf.reduce_mean(real_sample)\n",
    "    fake_loss = tf.reduce_mean(fake_sample)\n",
    "    #print('real_loss')\n",
    "    #print(real_loss)\n",
    "    #print('fake_loss')\n",
    "    #print(fake_loss)\n",
    "    loss = fake_loss - real_loss\n",
    "    #print('loss')\n",
    "    #print(loss.shape)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(batch_size, real_samples, fake_samples):\n",
    "    alpha = tf.random.uniform(shape = [batch_size, 1,1], minval = 0., maxval = 1.)\n",
    "    #print(real_samples.shape)\n",
    "    #print(fake_samples.shape)\n",
    "    \n",
    "    diff = fake_samples - real_samples\n",
    "    interp = real_samples + (alpha * diff)\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interp)\n",
    "        pred = discriminator(interp, training = True)\n",
    "    gradients = gp_tape.gradient(pred, [interp])[0]\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis  = [1,2]))\n",
    "    return tf.reduce_mean((norm-1.0) ** 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_samples, dpg = 5):\n",
    "    batch_size = 64\n",
    "    for i in range(dpg):\n",
    "        random_latent_vectors = tf.random.normal(shape = (batch_size, 100))\n",
    "        with tf.GradientTape() as tape:\n",
    "            fake_samples = generator(random_latent_vectors, training = True)\n",
    "            fake_logits = discriminator(fake_samples, training = True)\n",
    "            real_logits = discriminator(real_samples, training = True)\n",
    "            d_loss = discriminator_loss(real_logits, fake_logits)\n",
    "            \n",
    "            gp = gradient_penalty(batch_size, real_samples, fake_samples)\n",
    "            d_loss = d_loss + (gp * 10)\n",
    "            \n",
    "        d_gradients = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "        discriminator_optimizer.apply_gradients(zip(d_gradients, discriminator.trainable_variables))\n",
    "    \n",
    "\n",
    "    random_latent_vectors = tf.random.normal(shape = (batch_size, 100))\n",
    "    with tf.GradientTape() as tape:\n",
    "        gen_samples = generator(random_latent_vectors, training = True)\n",
    "        gen_logits = discriminator(gen_samples, training = True)\n",
    "        g_loss = generator_loss(gen_logits)\n",
    "\n",
    "    g_gradients = tape.gradient(g_loss, generator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(g_gradients, generator.trainable_variables))\n",
    "    print(\"Discriminator Loss:\")\n",
    "    tf.print(d_loss, output_stream=sys.stderr)\n",
    "    print(\" Loss:\")\n",
    "    tf.print(g_loss, output_stream=sys.stderr)\n",
    "    return {\"d_loss\":d_loss, \"g_loss\": g_loss}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch #\", epoch)\n",
    "        for batch in dataset:\n",
    "            train_step(batch)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = load_data(\"data.json\")\n",
    "\n",
    "\n",
    "#train(X, 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2 (None, 16, 16)\n3 (None, 64, 64)\n4 (None, 256, 4)\n5 (None, 1024, 2)\n6 (None, 4096, 1)\n7 (None, 16384, 1)\n0 (None, 16384, 1)\n0 (None, 4096, 1)\n0 (None, 1024, 2)\n0 (None, 256, 4)\n0 (None, 64, 8)\n0 (None, 16, 16)\n0 (None, 256)\n0 (None, 1)\n"
    }
   ],
   "source": [
    "#print(X.dtype)\n",
    "generator = makeGen()\n",
    "discriminator = makeDisc()\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.5, beta_2=0.9)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4, beta_1=0.5, beta_2=0.9)\n",
    "train_dataset == \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.00070278]\n [0.21556064]\n [0.3303949 ]\n ...\n [0.        ]\n [0.        ]\n [0.        ]]\n"
    }
   ],
   "source": [
    "from scipy.io.wavfile import write as wavwrite\n",
    "\n",
    "#print(X.shape)\n",
    "#print(np.squeeze(X, axis = 3).shape)\n",
    "##print(X)\n",
    "squeezed = np.squeeze(X,axis = 2)\n",
    "print(squeezed[0])\n",
    "wavwrite(\"first.wav\", 16000, squeezed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = tf.random.normal([1, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = generator(seed, training = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(1, 16384, 1), dtype=float32, numpy=\narray([[[0.00057547],\n        [0.        ],\n        [0.00034705],\n        ...,\n        [0.00072842],\n        [0.        ],\n        [0.00035185]]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import write as wavwrite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1, 16384, 1)\n(1, 16384)\n"
    }
   ],
   "source": [
    "print(pred.shape)\n",
    "pred = np.squeeze(pred.numpy(), axis =2)\n",
    "print(pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavwrite(\"preview2.wav\", 16000, pred.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.3.0\n"
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}